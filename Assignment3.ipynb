{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexallen7/AH2179/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YIbPBmGqxh00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_df = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\",sep=\";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2IYSKJGoy4T4"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
        "data_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted DataFrame\n",
        "days = np.unique(data_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days\n",
        "ndays = len(days)\n",
        "\n",
        "# Group the DataFrame 'data_df' by the \"Date\" column\n",
        "day_subsets_df = data_df.groupby([\"Date\"])\n",
        "\n",
        "# Define the total number of 5-minute intervals in a day\n",
        "nintvals = 288\n",
        "\n",
        "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
        "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
        "vectorized_day_dataset.fill(np.nan)\n",
        "\n",
        "# Loop through each unique day\n",
        "for i in range(0, ndays):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame\n",
        "        df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
        "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Iz1oAfKR_LM0",
        "outputId": "892b31b2-30d9-406f-b517-537460d23ffa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "nans_per_time = np.sum(np.isnan(vectorized_day_dataset),0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Create an array 'x_axis' representing the 5-minute intervals\n",
        "x_axis = np.arange(0, nintvals, 1, dtype=int)\n",
        "# Initialize an empty list 'x_axis_hours' to store time values in hours\n",
        "x_axis_hours = []\n",
        "# Convert interval indices to hours and append them to 'x_axis_hours'\n",
        "for i in range(0, len(x_axis)):\n",
        "  x_axis_hours.append(float(x_axis[i]*5)/60)\n",
        "ax.bar(x_axis_hours,height=nans_per_time)\n",
        "\n",
        "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aiYId1bLLeBr"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Create an array 'day_of_week' to store the day of the week for each unique date\n",
        "day_of_week = np.zeros((ndays))\n",
        "\n",
        "# Loop through each unique date\n",
        "for i in range(0, ndays):\n",
        "    # Parse the current date from a string to a datetime object\n",
        "    day_dt = datetime.datetime.strptime(str(days[i]), '%Y%m%d')\n",
        "\n",
        "    # Get the day of the week (1 for Monday, 2 for Tuesday, ..., 7 for Sunday)\n",
        "    day_of_week[i] = day_dt.isoweekday()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MOfS0A8Xt6mh"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "n_clusters = 2\n",
        "clusters = None\n",
        "#print(np.where(nans_per_day > 0)[0])\n",
        "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "# BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "#clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "# clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "# clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "if clusters is not None:\n",
        "  cluster_labels = clusters.labels_\n",
        "\n",
        "cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "# Calculate the Silhouette Score\n",
        "SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
        "\n",
        "# Calculate the Davies-Bouldin Score\n",
        "DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
        "\n",
        "# Calculate the Calinski-Harabasz Score\n",
        "CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PPwbOdp1voEh"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
        "n_clusters_t = len(np.unique(cluster_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1dqEbTgZfnx",
        "outputId": "2b29b2be-8a77-43ef-b7a3-a007ed098f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[35. 29. 32. ... 62. 66. 71.]\n",
            " [44. 44. 51. ... 30. 31. 23.]\n",
            " [21. 22. 17. ... 20. 22. 22.]\n",
            " ...\n",
            " [17. 17. 20. ... 35. 25. 27.]\n",
            " [37. 25. 30. ... 37. 36. 49.]\n",
            " [42. 28. 32. ... 45. 49. 38.]]\n"
          ]
        }
      ],
      "source": [
        "# Read the evaluation dataset from a CSV file\n",
        "data_eval_df = pd.read_csv(\"evaluation_dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
        "\n",
        "# Sort the evaluation DataFrame by columns \"Date\" and \"Interval_5\"\n",
        "data_eval_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted evaluation DataFrame\n",
        "days_eval = np.unique(data_eval_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days in the evaluation dataset\n",
        "ndays_eval = len(days_eval)\n",
        "\n",
        "# Group the evaluation DataFrame by the \"Date\" column\n",
        "day_eval_subsets_df = data_eval_df.groupby([\"Date\"])\n",
        "\n",
        "# Initialize a matrix 'vectorized_day_dataset_eval' filled with NaN values\n",
        "vectorized_day_dataset_eval = np.zeros((ndays_eval, nintvals))\n",
        "vectorized_day_dataset_eval.fill(np.nan)\n",
        "# This section initializes a 2D array to store the evaluation dataset and fills it with NaN values.\n",
        "\n",
        "# Loop through each unique day in the evaluation dataset\n",
        "for i in range(0, ndays_eval):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame (this line is redundant)\n",
        "        df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset_eval'\n",
        "        vectorized_day_dataset_eval[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset_eval'\n",
        "print(vectorized_day_dataset_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nNUDJ57Uj7JZ"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of days with missing values\n",
        "nans_per_day_eval = np.sum(np.isnan(vectorized_day_dataset_eval), 1)\n",
        "\n",
        "\n",
        "# Filter out days with no missing values and create a new dataset\n",
        "vectorized_day_dataset_no_nans_eval = vectorized_day_dataset_eval[np.where(nans_per_day_eval == 0)[0], :]\n",
        "days_not_nans_eval = days_eval[np.where(nans_per_day_eval == 0)[0]]\n",
        "\n",
        "# Calculate the total number of days in the filtered evaluation dataset\n",
        "ndays_eval_not_nans = len(days_not_nans_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6259sc-sgVE8",
        "outputId": "4aea40fe-889a-45b3-9f4e-59422bbf2650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction accuracy MAE: 32.57421525988162\n",
            "Prediction accuracy MAPE: 0.44248197239664977\n",
            "Silhouette Score: 0.30762127308637705\n",
            "Davies-Bouldin Score: 1.2677249598188514\n",
            "Calinski-Harabasz Score: 174.1048835493247\n"
          ]
        }
      ],
      "source": [
        "# Import the pairwise_distances function from scikit-learn's metrics library\n",
        "import sklearn.metrics.pairwise as dis_lib\n",
        "\n",
        "# Define a function to find the closest centroid to a new data point within a specified day-time interval range\n",
        "def find_the_closest_centroid(centroids, new_day, from_interval: int, to_interval: int):\n",
        "    closest_centroid = None\n",
        "    closest_dist = None\n",
        "\n",
        "    # Iterate through each centroid\n",
        "    for i in range(0, len(centroids)):\n",
        "        # Calculate the Euclidean distance between the centroid and the new data point\n",
        "        ed_t = dis_lib.paired_distances(centroids[i], new_day, metric='euclidean')\n",
        "\n",
        "        # Check if the current centroid is closer than the previously closest one\n",
        "        if closest_centroid is None or closest_dist > ed_t:\n",
        "            closest_centroid = i\n",
        "            closest_dist = ed_t\n",
        "\n",
        "    return closest_centroid\n",
        "\n",
        "# Initialize a list to store centroid data\n",
        "centroids = []\n",
        "\n",
        "# Calculate centroids for each cluster\n",
        "for i in np.unique(cluster_labels):\n",
        "    centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
        "    centroids.append(centroid)\n",
        "\n",
        "# Define the number of past intervals to consider for classification\n",
        "n_past_intervals_for_classification = 5\n",
        "\n",
        "# Initialize variables to calculate accuracy metrics\n",
        "total_mae = 0\n",
        "total_mape = 0\n",
        "prediction_counts = 0\n",
        "\n",
        "# Loop through each day in the evaluation dataset with no missing values\n",
        "for i in range(0, ndays_eval_not_nans):\n",
        "    # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "    for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "        # Find the closest centroid for the current data point\n",
        "        centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "        # Predict the value for the next interval\n",
        "        predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "        mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "        mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "\n",
        "        # Accumulate MAE, MAPE, and count of predictions\n",
        "        total_mae += mae_t\n",
        "        total_mape += mape_t\n",
        "        prediction_counts += 1\n",
        "\n",
        "# Calculate and print the prediction accuracy metrics\n",
        "print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
        "print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n",
        "\n",
        "print('Silhouette Score:', SC_score)\n",
        "print('Davies-Bouldin Score:', DB_score)\n",
        "print('Calinski-Harabasz Score:', CH_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWhUXUPKbphz"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}